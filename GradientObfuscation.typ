= Graident Obfuscation
 
“梯度遮蔽 (Gradient Obfuscation)”是 Athalye, Carlini, Wagner 2018 那篇经典论文的主题，他们总结了几类看似有效、实则无效的防御机制。

== 梯度遮蔽 (Gradient Masking / Obfuscation)

含义：模型的输入梯度（loss 对输入的导数）被“隐藏”或“干扰”，使得常规基于梯度的攻击（如 FGSM、PGD）失效。

表现形式：

梯度趋近于 0（比如用强烈的饱和激活函数，梯度全消失）；

梯度方向被 噪声 或 不规则函数 扰乱，攻击者算出来的方向没意义；

训练让梯度在局部呈现锯齿状、不连续。

问题：这并不是“真正更鲁棒”，而是攻击者找不到有用梯度；一旦换用更强或不用梯度的攻击（黑盒搜索、BPDA 等），防御立即失效。

== 随机性 (Stochastic Defenses)

含义：在前向传播时加入随机扰动或随机层，希望让攻击梯度不稳定。

例子：随机输入变换（如加噪声、随机缩放、随机填充）、随机 Dropout-like 层。

问题：攻击者可以用 EOT (Expectation Over Transformation) 技术，把随机性“平均”掉，依然获得有效的梯度，从而攻破防御。

== 不可微操作 (Non-differentiable Components)

含义：在模型前面/里面加入不可微的操作，比如取整、符号函数、JPEG 压缩、量化等，导致攻击时梯度为 0 或未定义。

例子：

“Input quantization” 把像素四舍五入到固定值；

“JPEG defense” 把图像先 JPEG 压缩再送入网络。

问题：攻击者可以用 BPDA (Backward Pass Differentiable Approximation)：前向时用真实不可微操作，反向传播时用一个可微近似来传梯度，就能绕过。

== min-max 训练 (Adversarial Training)

=== 理论视角：
最小—最大（saddle-point）问题

PGD 的角色：PGD 是一种一阶方法（基于输入梯度）来近似内层最大化：多步梯度上升 + 每步投影回允许集 S。在非凸损失下它不是全局最优，但在实践中（尤其带随机初始化）通常能找到很强的攻击，是经验上可靠的内层近似。

与分布式/概率鲁棒的联系：可被看成对某类局部扰动集合的worst-case分布进行优化，和一般的期望风险最小化形成对照。

=== 可靠做法
被声明的威胁模型（不是靠不可微/随机技巧去隐藏梯度），因此在白盒环境下用 PGD 等强攻击评测不会“假鲁棒”地过分乐观。

Athalye 等人在 2018 年揭露的“梯度遮蔽”并不能绕过这类方法——因为对抗训练本身需要并依赖梯度进行内层优化。

在实践中，这类训练确实能明显提升在同一威胁模型下的鲁棒性（比未训练模型强很多），是很多后续工作以之为基线的“正统路线”。

- 简单直接：只需把训练中插入内层 PGD 循环即可实现。

- 通用：可用于不同网络、不同威胁集

- 可扩展到各种改进